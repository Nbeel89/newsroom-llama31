model_id: meta-llama/Meta-Llama-3.1-8B-Instruct

dataset_path: data/train/newsroom_v2_train.jsonl
output_dir: outputs/llama31_lora_v2

max_seq_len: 256      # ⬅ smaller context = much faster

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

learning_rate: 0.0001

per_device_train_batch_size: 1   # keep tiny for 6GB VRAM
gradient_accumulation_steps: 4   # ⬅ small, so steps move quickly
num_train_epochs: 1              # ⬅ start with 1 epoch to test

warmup_ratio: 0.03
weight_decay: 0.0

logging_steps: 1
report_to: none
