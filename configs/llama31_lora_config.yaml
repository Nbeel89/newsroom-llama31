# LLaMA 3.1 8B LoRA fine-tuning config
# FINAL VERSION

# ----- Base model & data -----
model_id: "meta-llama/Meta-Llama-3.1-8B-Instruct"
dataset_path: "data/cleaned/newsroom_finetune.jsonl"
output_dir: "outputs/llama31_lora_v1"

# Keep this moderate for stability on local GPU
max_seq_length: 512

# ----- Quantization / loading -----
# Use 4-bit so the 8B model fits on GPU
load_in_4bit: true

# ----- LoRA configuration -----
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

bias: "none"
task_type: "CAUSAL_LM"

# ----- Training hyperparameters -----
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
weight_decay: 0.0

lr_scheduler_type: "cosine"
warmup_ratio: 0.03

max_grad_norm: 1.0
seed: 42

# ----- Logging / saving -----
logging_steps: 25
save_steps: 200
save_total_limit: 2

# ----- Precision flags -----
bf16: false     # RTX 4050/consumer GPUs: use fp16 instead
fp16: true

# We will NOT use gradient checkpointing here
gradient_checkpointing: false

report_to: "none"
