# Newsroom LLaMA 3.1 LoRA Fine-Tuning Pipeline

A complete, lightweight, and production-ready pipeline for fine-tuning
**Meta LLaMA-3.1-8B-Instruct** using **LoRA**, optimized for generating
**Al Jazeera--style newsroom summaries** (headline, lede, and factual
bullets).

This repository includes everything end-to-end: - ğŸ” Web crawling (RSS +
sitemaps + sections) - ğŸ§¹ Text cleaning & transformation - ğŸ“˜ Dataset
preparation for supervised fine-tuning - ğŸ§  QLoRA training pipeline - ğŸ“Š
Evaluation (baseline vs LoRA) - âš¡ Local inference engine - ğŸ–¥ï¸ Minimal
Flask console UI

## ğŸš€ Features

-   **Deep Article Crawl** (RSS feeds + sitemap discovery + section
    pages)\
-   **Robust Cleaning** (HTML removal, normalization, deduplication,
    min-word filters)\
-   **Training** using QLoRA (4-bit quantization) for cost-efficient
    fine-tuning\
-   **Evaluation Scripts** with outputs stored in `data/eval`\
-   **Local Inference** using adapter injection\
-   **Modular Structure** --- configurable through YAML files

## ğŸ“‚ Project Structure

    newsroom-llama31/
    â”‚   README.md
    â”‚   requirements.txt
    â”‚   .gitignore
    â”‚
    â”œâ”€â”€ configs/
    â”‚   â”œâ”€â”€ llama31_inference.yaml
    â”‚   â””â”€â”€ llama31_lora_config.yaml
    â”‚
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ cleaned/
    â”‚   â””â”€â”€ eval/
    â”‚
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ 01_crawl_articles.py
    â”‚   â”œâ”€â”€ 02_clean_articles.py
    â”‚   â”œâ”€â”€ 03_prepare_dataset.py
    â”‚   â”œâ”€â”€ 04_train_lora_llama.py
    â”‚   â”œâ”€â”€ 05_evaluate_lora.py
    â”‚   â”œâ”€â”€ 06_inference_llama.py
    â”‚   â”œâ”€â”€ 07_run_flask_console.py
    â”‚   â”œâ”€â”€ 08_batch_eval.py
    â”‚   â”œâ”€â”€ 10_prepare_eval_sheet.py
    â”‚   â””â”€â”€ crawl_site_full_v4.py
    â”‚
    â””â”€â”€ static/
        â”œâ”€â”€ css/
        â””â”€â”€ img/

## âš™ï¸ Installation

``` bash
git clone https://github.com/Nbeel89/newsroom-llama31.git
cd newsroom-llama31
pip install -r requirements.txt
```

Requirements: - Python 3.10+ - CUDA GPU recommended (for training) -
HuggingFace login (if model requires authentication)

## ğŸ•¸ï¸ 1. Crawl News Articles

``` bash
python scripts/01_crawl_articles.py   --days 3650   --max-pages 50   --workers 6   --min-words 80   --out data/raw/articles.jsonl
```

## ğŸ§¹ 2. Clean Articles

``` bash
python scripts/02_clean_articles.py   --input data/raw/articles.jsonl   --output data/cleaned/aljazeera_articles_cleaned.jsonl
```

## ğŸ“˜ 3. Prepare Fine-Tuning Dataset

``` bash
python scripts/03_prepare_dataset.py   --input data/cleaned/aljazeera_articles_cleaned.jsonl   --output data/cleaned/newsroom_finetune.jsonl
```

## ğŸ§  4. Train LoRA Adapter on LLaMA-3.1-8B

``` bash
python scripts/04_train_lora_llama.py   --config configs/llama31_lora_config.yaml
```

Outputs appear in:

    outputs/llama31_lora_v1/

## ğŸ“Š 5. Evaluate Model (Baseline vs LoRA)

``` bash
python scripts/05_evaluate_lora.py
```

Evaluation results are stored under:

    data/eval/

## ğŸ” 6. Run Local Inference

``` bash
python scripts/06_inference_llama.py   --config configs/llama31_inference.yaml
```

## ğŸ–¥ï¸ 7. Optional: Run the Flask Console UI

``` bash
python scripts/07_run_flask_console.py
```

This opens a minimal local web interface to test summaries.

## ğŸ§¾ License

MIT License --- free for use, modification, and commercial work.

## â­ Acknowledgments

-   Meta --- LLaMA 3.1\
-   HuggingFace Transformers\
-   LoRA (Hu et al.Â 2021)\
-   QLoRA (Dettmers et al., 2023)\
-   Al Jazeera article dataset generated via public web content
